<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="None">
        
        
        <link rel="shortcut icon" href="img/favicon.ico">
        <title>Microsoft Applied Robotics Research Library</title>
        <link href="css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="css/font-awesome.min.css" rel="stylesheet">
        <link href="css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="js/jquery-1.10.2.min.js" defer></script>
        <script src="js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body class="homepage">

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <a class="navbar-brand" href=".">Microsoft Applied Robotics Research Library</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#microsoft-applied-robotics-research-library">Microsoft Applied Robotics Research Library</a></li>
            <li><a href="#open-source-samples-for-service-robotics">Open Source Samples for Service Robotics</a></li>
        <li class="main "><a href="#human-robot-interaction-hri">Human-Robot Interaction (HRI)</a></li>
            <li><a href="#labanotation-suite-repository">Labanotation Suite repository</a></li>
            <li><a href="#gesturebot-design-kit-repository">gestureBot Design Kit repository</a></li>
        <li class="main "><a href="#navigation">Navigation</a></li>
            <li><a href="#hololensnavigationforrobots-repository">HoloLensNavigationForRobots repository</a></li>
        <li class="main "><a href="#manipulation">Manipulation</a></li>
        <li class="main "><a href="#additional-microsoft-resources-for-robotics">Additional Microsoft Resources for Robotics</a></li>
            <li><a href="#httpsdevelopermicrosoftcomen-uswindowsiot">https://developer.microsoft.com/en-us/windows/iot/</a></li>
            <li><a href="#httpsgithubcommicrosoftairsim">https://github.com/microsoft/AirSim</a></li>
            <li><a href="#httpsms-iotgithubiorosonwindows">https://ms-iot.github.io/ROSOnWindows/</a></li>
            <li><a href="#httpsgithubcomms-iot">https://github.com/ms-iot</a></li>
            <li><a href="#httpsgithubcommicrosoftai-robot-challenge-lab">https://github.com/Microsoft/AI-Robot-Challenge-Lab</a></li>
            <li><a href="#httpsgithubcomms-iotholobot">https://github.com/ms-iot/holobot</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h2 id="microsoft-applied-robotics-research-library"><strong><img alt="logo" src="MARR_logo.png" /> Microsoft Applied Robotics Research Library</strong></h2>
<h3 id="open-source-samples-for-service-robotics">Open Source Samples for Service Robotics</h3>
<p><a href="https://opensource.org/licenses/MIT"><img alt="License: MIT" src="https://img.shields.io/badge/License-MIT-yellow.svg" /></a>  </p>
<p>This library is a collection of GitHub repositories containing software applications, data sets, and hardware reference designs to support research and education in the fields of <strong>service-robotics</strong>. The term <strong>service-robotics</strong> refers to the expectation that the robots can safely operate in dynamic environments and in close proximity with humans. This is in contrast to <strong>industrial-robotics</strong> which often refers to systems performing pre-programmed repetitive tasks inside a cordoned-off safety zone.   Our team invites you to join us in research and engineering efforts that advance how robots can assist and augment the capabilities of human beings.</p>
<p>Projects are currently available in the fields of <strong>Human-Robot-Interaction</strong> with the goal of providing service robots the means to effectively communicate with humans, and robot <strong>Navigation</strong> with the goal of helping robots find their way to a destination. In the near future we plan to offer new repositories exploring the field of physical object <strong>Manipulation</strong>.</p>
<h1 id="human-robot-interaction-hri">Human-Robot Interaction (HRI)</h1>
<p>The field of HRI includes robot speech, expressions, gestures, and information access to provide natural user interface methods for robot applications.</p>
<h2 id="labanotation-suite-repository"><a href="https://microsoft.github.io/LabanotationSuite"><strong>Labanotation Suite</strong></a> <a href="https://github.com/microsoft/LabanotationSuite"><em>repository</em></a></h2>
<p>The Labanotation Suite is a collection of applications comprising a system that can be used to give service robots the ability to move in natural and meaningful ways. It includes software tools, source code, sample data, and hardware simulation software that supports experimentation with the concepts presented in the paper <strong><a href="https://link.springer.com/article/10.1007%2Fs11263-018-1123-1">Describing Upper-Body Motions Based on Labanotation for Learning-from-Observation Robots</a> (International Journal of Computer Vision, December 2018)</strong>. The system consists of compiled gesture capture applications using the Microsoft Kinect sensor device and a Windows 10 PC. Editing tools constructed in Python provide gesture trimming and movement analysis options to identify key points of movment. Output is graphical Labanotation scores as well as movement data expressed in Labanotation and stored in JSON data format. </p>
<h3 id="kinectreader-human-gesture-capture-tool"><strong><a href="https://github.com/microsoft/LabanotationSuite/tree/master/GestureAuthoringTools/KinectReader">KinectReader: </a> Human Gesture Capture Tool</strong></h3>
<p>KinectReader is a compiled Windows application that connects to a Kinect sensor device and provides a user interface for capturing and storing gestures performed by human subjects. It's primary output data is human stick-figure joint positions in a CSV format, but can also capture corresponding RGB video and audio at the same time.</p>
<h3 id="kinectcaptureeditor-human-gesture-trimming-tool"><strong><a href="https://github.com/microsoft/LabanotationSuite/tree/master/GestureAuthoringTools/KinectCaptureEditor">KinectCaptureEditor: </a> Human Gesture Trimming Tool</strong></h3>
<p>KinectCaptureEditor is a compiled Windows application that loads human joint position CSV files produced by the KinectReader or other tools, as well as optional corresponding video and audio files. It provides a timeline-based method to trim audio and video joint movement sequences into representative human gestures.</p>
<h3 id="labaneditor-gesture-analysis-and-labanotation-generator"><strong><a href="https://github.com/microsoft/LabanotationSuite/tree/master/GestureAuthoringTools/LabanEditor">LabanEditor: </a>  Gesture Analysis and Labanotation Generator</strong></h3>
<p>LabanEditor is a Python application that loads a Kinect joint CSV file representing a human gesture, provides algorithmic options for automatically extracting keyframes from the gesture that correspond Labanotation data, and provides a graphical user interface for selection and modification of the extracted keyframes. Additionally, it saves the resulting gesture data in a JSON file format suitable for controlling robots running a gesture interpretation driver, as well as PNG graphic file renderings of the charts and diagrams used in the interface.</p>
<h3 id="msrabotsimulation-gesture-performance-with-simulated-robot"><strong><a href="https://github.com/microsoft/LabanotationSuite/tree/master/MSRAbotSimulation">MSRAbotSimulation: </a>  Gesture Performance with Simulated Robot</strong></h3>
<p>This Python and browser-based simulation software uses javascript and html code to implement an animated 3D model of the robot and a user interface for selecting and rendering gestures described in the JSON format. A temporary local HTTP server invoked with python or an existing server can be used to host the software and the simulation is run within a modern web browser. The user can choose from a collection of sample gestures, or select a new gesture captured and created using this project's Gesture Authoring Tools.</p>
<h2 id="gesturebot-design-kit-repository"><a href="https://microsoft.github.io/gestureBotDesignKit/"><strong>gestureBot Design Kit</strong></a> <a href="https://github.com/microsoft/gestureBotDesignKit"><em>repository</em></a></h2>
<p>With a Windows 10 PC, and optionally a 3D-printer and about $350(USD) in electronic servos and parts, the gestureBot Design Kit repository contains all the information needed to build both a virtual and physical desktop companion robot. It includes browser-based simulation and control software based on the <a href="https://www.robotis.us/dynamixel-xl-320/">Robotis XL</a> series of servo motors. To construct a physical robot, it provides models for 3D-printable body-parts, a parts-list for electronic components, and step-by-step assembly instructions. No soldering is required, but some manual skill is needed to mate small electronic connectors, as well as manipulate small plastic rivets and miniature metal screws to assemble the body components.</p>
<h3 id="gesture-library-example-set-of-upper-torso-gestures"><strong><a href="https://github.com/microsoft/gestureBotDesignKit/tree/main/src/Labanotation">Gesture Library: </a> Example Set of Upper-Torso Gestures</strong></h3>
<p>The Gesture Library is a data-set of upper-torso gesture-concept pairs expressed in Labanotation format and stored as JSON files. The data is directly accessed by the Gesture Service and is organized around 40 clusters of gesture-concept pairs including 6 deictic concepts (me, you, this, that, here, there), 33 expressive theme concepts (hello, many, question, etc.), and 1 "beat" concept used for idling. The clusters were segregated using a method described in the paper: <a href="https://hal.archives-ouvertes.fr/hal-03108169"><strong><em>Development and Verification of a Gesture-generating Architecture for Conversational Humanoid Robots:</em></strong> </a>. The library includes a complete listing of the sample data including a video clip of each gesture performed by the gestureBot.</p>
<h3 id="gesture-service-example-gesture-service-engine"><strong><a href="https://github.com/microsoft/gestureBotDesignKit/tree/main/src/Samples/gestureService_w2v">Gesture Service: </a> Example Gesture Service Engine</strong></h3>
<p>The Gesture Service project is a software module constructed with Python and Google's neural network <a href="https://code.google.com/archive/p/word2vec/#!">word2vec</a> that takes a text phrase as input and returns a corresponding gesture.</p>
<h1 id="navigation">Navigation</h1>
<p>The field of robot navigation includes systems and methods such as simultaneous-location-and-mapping (SLAM), path planning, and map management.</p>
<h2 id="hololensnavigationforrobots-repository"><a href="https://microsoft.github.io/HoloLensNavigationForRobots"><strong>HoloLensNavigationForRobots</strong></a> <a href="https://github.com/microsoft/HoloLensNavigationForRobots"><em>repository</em></a></h2>
<p>The HoloLensNavigation system shows how a <a href="https://www.microsoft.com/en-us/hololens">HoloLens</a> device can be placed on the head of <a href="https://us.softbankrobotics.com/pepper">Pepper robot</a> and provide a self-calibrating indoor navigation solution within a single room. The calibration process is described in the paper: <a href="https://www.cvl.iis.u-tokyo.ac.jp/data/uploads/papers/Ishikawa_SLAMDevice_ROMAN2019.pdf"><strong><em>Dynamic Calibration between a Mobile Robot and SLAM Device for Navigation</em></strong></a>. The system operates in and provides examples of three modes of operation supporting navigation: map generation, position calibration, and movement to a goal position.</p>
<h3 id="hololensspatialmapping-dynamic-spatial-mapping"><strong><a href="https://github.com/microsoft/HololensNavigationForRobots/tree/main/windows">HoloLensSpatialMapping: </a> Dynamic Spatial Mapping</strong></h3>
<p>HololensSpatialMapping is a UWP application that uses the HoloLens device sensors to capture and maintain a spatial map of the immediate environment and also communicates with HoloROS Bridge.</p>
<h3 id="hololens_localization-local-position-calibration-and-computation"><strong><a href="https://github.com/microsoft/HololensNavigationForRobots/tree/main/linux/HoloLens_Localization">HoloLens_Localization: </a> Local Position Calibration and Computation</strong></h3>
<p>HoloLens_Localization is a ROS (Melodic) package that computes the local position of the robot based on sensor measurements as the robot moves through calibrated poses and navigates through the environment.</p>
<h3 id="holorosbridge-ros-communication-with-hololens-device"><strong><a href="https://github.com/microsoft/HololensNavigationForRobots/tree/main/linux/HoloROSBridge">HoloROSBridge: </a>ROS Communication with HoloLens Device</strong></h3>
<p>HoloROSBridge is a ROS (Melodic) package that communicates with the HoloLensSpatialMapping application running on the HoloLens device.</p>
<h3 id="holo_nav_dash-operational-dashboard"><strong><a href="https://github.com/microsoft/HololensNavigationForRobots/tree/main/linux/holo_nav_dash">holo_nav_dash: </a> Operational Dashboard</strong></h3>
<p>holo_nav_dash is a ROS (Melodic) package that provides a local http server and a browser-based operational interface for starting up and monitoring calibration and navigation operations.</p>
<h3 id="navigation_launcher-ros-navigation-stack-launcher"><strong><a href="https://github.com/microsoft/HololensNavigationForRobots/tree/main/linux/navigation_launcher">navigation_launcher: </a> ROS Navigation Stack Launcher</strong></h3>
<p>navigation_launcher is a ROS (Melodic) package that contains launch scripts for starting up components for the HoloLens stack, the HoloLens Navigation stack, and the ROS Navigation stack.</p>
<h1 id="manipulation">Manipulation</h1>
<p>In industrial applications, robotic object manipulation is common where actions are manually programmed and repeated behind safety barriers. In service-robotics scenarios, dynamic environments and safety considerations make the entire field much more challenging. Our projects explore solutions where HRI and Navigation technologies can be leveraged to allow robots to learn from humans to perform manipulation tasks safely and effectively in residential, workplace, and public environments. </p>
<p>While we are working towards an open-source object-manipulation sample for service-robotics next year, our team-mates at <a href="https://www.microsoft.com/en-us/ai/autonomous-systems-project-bonsai"><strong>Microsoft Project Bonsai</strong></a> are forging ahead with autonomous systems for industrial applications.  Take a look at <a href="https://microsoft.github.io/moab/"><strong><em>Project Moab</em></strong></a>.</p>
<h1 id="additional-microsoft-resources-for-robotics">Additional Microsoft Resources for Robotics</h1>
<p>There's a lot going on at Microsoft in robotics and other intelligent technologies. We invite you to check out these links to learn more:</p>
<h2 id="httpsdevelopermicrosoftcomen-uswindowsiot">https://developer.microsoft.com/en-us/windows/iot/</h2>
<p><strong>Windows 10 IoT</strong> is a member of the Windows 10 product family that brings enterprise-class power, security, and manageability to the Internet of Things. Today, there are over 10,000 Windows IoT partners from the Edge to the Azure Cloud.</p>
<h2 id="httpsgithubcommicrosoftairsim">https://github.com/microsoft/AirSim</h2>
<p>From Microsoft AI &amp; Research, open source simulator for autonomous vehicles built on Unreal / Unity physics-modeling engines.</p>
<h2 id="httpsms-iotgithubiorosonwindows">https://ms-iot.github.io/ROSOnWindows/</h2>
<p><a href="https://microsoft.github.io/Win-RoS-Landing-Page/"><strong>Microsoft supports</strong></a> the <a href="https://www.ros.org"><strong>Robot Operating System (ROS)</strong></a> framework for collaborative, open source robotic software development.</p>
<h2 id="httpsgithubcomms-iot">https://github.com/ms-iot</h2>
<p>The Microsot IoT public repositories on GitHub contain more than 200 projects related to Microsoft technologies, automation, intelligent environments, and robotics.</p>
<h2 id="httpsgithubcommicrosoftai-robot-challenge-lab">https://github.com/Microsoft/AI-Robot-Challenge-Lab</h2>
<p>This laboratory guide walks a user through setting up voice-control of a simulated robotic arm using the <a href="https://azure.microsoft.com/en-us/services/bot-services/"><strong>Microsoft Azure Bot Service</strong></a>.</p>
<h2 id="httpsgithubcomms-iotholobot">https://github.com/ms-iot/holobot</h2>
<p>For the 2015 Microsoft Build conference, the Microsoft Hololens team built this very interesting <a href="https://channel9.msdn.com/Events/Build/2015/KEY01"><strong>mixed-reality robot "B15"</strong></a> <em>(skip ahead to 2:47:00)</em> and shared their source code and <a href="https://www.youtube.com/watch?v=r1PaAWvygQk"><strong>their experiences building it</strong>.</a></p></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = ".",
                shortcuts = {"search": 83, "next": 78, "help": 191, "previous": 80};
        </script>
        <script src="js/base.js" defer></script>
        <script src="search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>

<!--
MkDocs version : 1.0.4
Build Date UTC : 2021-09-10 00:02:25
-->
